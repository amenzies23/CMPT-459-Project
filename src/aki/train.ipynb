{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c731117",
   "metadata": {},
   "source": [
    "# K-NN Training Overview\n",
    "This notebook implements a full supervised machine-learning pipeline for plant-health classification using k-Nearest Neighbors (k-NN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38846436",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from typing import List\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "from core.preprocessing import load_and_preprocess_data\n",
    "from core.outlier_detection import remove_outliers\n",
    "from core.feature_selection import select_features\n",
    "from core.classification import (\n",
    "    classify_with_knn,\n",
    "    classify_with_knn_without_hyperparameter,\n",
    ")\n",
    "from core.visualization import (\n",
    "    visualize_ground_truth,\n",
    "    visualize_knn_decision_boundary,\n",
    ")\n",
    "\n",
    "from utils import find_project_root\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f808dc09",
   "metadata": {},
   "source": [
    "## Setting Random Seeds\n",
    "Ensures that any randomness in preprocessing or clustering is reproducible.\n",
    "This allows consistent results when rerunning the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0189c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc07859",
   "metadata": {},
   "source": [
    "## Data Loading & Preprocessing\n",
    "Here, we load the plant-health dataset and perform basic preprocessing steps required before clustering.\n",
    "We:\n",
    "- Reads the CSV file.\n",
    "- Removes non-informative columns (`Timestamp`, `Plant_ID`).\n",
    "- Splits data into features (`X`) and labels (`y`).\n",
    "- Fills missing values (if any) using column means.\n",
    "- Encodes the target variable using `LabelEncoder` (for visualization).\n",
    "- Standardizes numerical features using `StandardScaler` and returns both raw and scaled versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5f4964",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = find_project_root()\n",
    "\n",
    "train_path = os.path.join(PROJECT_ROOT, \"data\", \"train_data.csv\")\n",
    "test_path = os.path.join(PROJECT_ROOT, \"data\", \"test_data.csv\")\n",
    "\n",
    "X_train_raw, X_test_raw, y_train, y_test, X_train_scaled, X_test_scaled, label_encoder, scaler = (\n",
    "    load_and_preprocess_data(\n",
    "        train_path,\n",
    "        test_path\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bce72e",
   "metadata": {},
   "source": [
    "## Outlier Removal\n",
    "Identify and remove anomalous samples from a dataset using the Local Outlier Factor (LOF) algorithm. \n",
    "It takes a scaled feature matrix (`X_scaled`) and corresponding labels (`y_encoded`), \n",
    "detects outliers based on how isolated each sample is compared to its local neighborhood, \n",
    "and returns a cleaned dataset with outliers removed.\n",
    "\n",
    "After detecting outliers: \n",
    " - Filters out all samples labeled as outliers\n",
    " - Returns the cleaned feature matrix and labels\n",
    " - Visualizes inliers and outliers using the first two features\n",
    "\n",
    "From the plot, the outliers (red Xs) appear randomly scattered away from high-density regions. This suggests random measurement errors or extreme anomalies \n",
    "as there is no visible structure or cluster pattern among the outliers.\n",
    "- They do not group into a separate meaningful cluster.\n",
    "- They do not represent a rare class.\n",
    "- They are unlikely to contain useful information\n",
    "\n",
    "Therefore, we decided to remove these found outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814e3110",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_clean, y_train_clean = remove_outliers(\n",
    "    X_train_scaled, # scaled version\n",
    "    y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a26436",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "Perform feature selection using the `SelectKBest` with `Mutual Information` as the scoring metric. \n",
    "It evaluates how informative each feature is in predicting the target labels, \n",
    "selects the top k most relevant features, and returns:\n",
    " - `X_selected`: the transformed dataset containing only the selected features\n",
    " - `selected_features`: the names of the chosen features\n",
    "\n",
    "This process helps reduce dimensionality, remove irrelevant inputs, \n",
    "and improve the performance and interpretability of ML models. \n",
    "\n",
    "Feature selection is a crucial step as without performing feature selection (see: `train_without_feature_selection.ipynb`), the model performance decreases approximately `5-10%`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a67251",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_selected, selected_features = select_features(\n",
    "    X_train_clean,\n",
    "    y_train_clean,\n",
    "    X_train_raw.columns,\n",
    "    k=8\n",
    ")\n",
    "\n",
    "# Apply same feature selection to test set\n",
    "X_test_selected = pd.DataFrame(\n",
    "    scaler.transform(X_test_raw),\n",
    "    columns=X_train_raw.columns\n",
    ")[selected_features].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d911ad",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "Two visualizations are provided:\n",
    "- Ground truth scatter of selected features\n",
    "- k-NN decision boundary plot (using 2D PCA) to understand classifier behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc63de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_scaler = StandardScaler()\n",
    "X_train_selected_scaled = selected_scaler.fit_transform(X_train_selected)\n",
    "\n",
    "visualize_ground_truth(X_train_selected_scaled, y_train_clean, label_encoder)\n",
    "visualize_knn_decision_boundary(X_train_selected_scaled, y_train_clean, n_neighbors=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e091967",
   "metadata": {},
   "source": [
    "## Classification\n",
    "Here we train `K-NN` classifier with hyperparameter tuning where\n",
    "we search for optimal `k`.\n",
    "\n",
    "With `k=8` selected features and hyperparameter tuning, the K-NN classifier reaches about `76%` accuracy, compared to only `70%` without tuning (using `classify_with_knn_without_hyperparameter`). When hyperparameter tuning is applied and feature selection is not used, the classifier performs worse, achieving around `69%`. The lowest accuracy appears when training K-NN without either hyperparameter tuning or feature selection, giving only `65%`, which is only slightly better than random guessing.\n",
    "\n",
    "However, when the number of selected features is changed to `k=4`, the modelâ€™s accuracy improves significantly to about `88%`.\n",
    "\n",
    "This shows that both hyperparameter optimization and feature selection provide important performance gains for the `K-NN` classifier. \n",
    "\n",
    "Additionally,\n",
    "\n",
    "## ROC Curve\n",
    "From the ROC Curve, the classifier distinguishes `Healthy` and `High Stress` extremely well, but is less confident when detecting `Moderate Stress`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88580f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train classifier with hyperparameter\n",
    "print(\"Running KNN with hyperparameter tuning...\")\n",
    "knn = classify_with_knn(\n",
    "    X_train_selected_scaled,\n",
    "    y_train_clean,\n",
    "    label_encoder,\n",
    "    X_test=X_test_selected,\n",
    "    y_test=y_test\n",
    ")\n",
    "\n",
    "# knn = classify_with_knn_without_hyperparameter(\n",
    "#     X_train_selected_scaled,\n",
    "#     y_train_clean,\n",
    "#     label_encoder,\n",
    "#     X_test=X_test_selected,\n",
    "#     y_test=y_test\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
